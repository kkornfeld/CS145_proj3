vllm serve meta-llama/Llama-3.2-3B-Instruct --gpu_memory_utilization=0.85 --tensor_parallel_size=1 --dtype="half" --port=8088 --enforce_eager --max_model_len=16384
python generate.py --dataset_path "data/crag_task_1_dev_v4_release.jsonl.bz2" --split 1 --model_name "rag_baseline" --llm_name "meta-llama/Llama-3.2-3B-Instruct" --is_server --vllm_server "http://localhost:8088/v1"
python evaluate.py --dataset_path "data/crag_task_1_dev_v4_release.jsonl.bz2" --model_name "rag_baseline" --llm_name "meta-llama/Llama-3.2-3B-Instruct" --is_server --vllm_server "http://localhost:8088/v1" --max_retries 10